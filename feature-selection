import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import VarianceThreshold, RFECV
from sklearn.metrics import accuracy_score

diabetes_data = pd.read_csv('diabetes.csv')


missing_diabetes = diabetes_data.isnull().mean()
diabetes_cleaned = diabetes_data.loc[:, missing_diabetes < 0.3]


X_diabetes = diabetes_cleaned.drop(columns=['Outcome'])
y_diabetes = diabetes_cleaned['Outcome']
X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(X_diabetes, y_diabetes, test_size=0.3, random_state=42)

model_diabetes = LogisticRegression(max_iter=200)
model_diabetes.fit(X_train_diabetes, y_train_diabetes)
y_pred_diabetes = model_diabetes.predict(X_test_diabetes)
accuracy_diabetes = accuracy_score(y_test_diabetes, y_pred_diabetes)
print(f'Accuracy after Missing Value Filter (Diabetes): {accuracy_diabetes:.4f}')


correlation_matrix_diabetes = diabetes_cleaned.corr()
high_corr_pairs_diabetes = correlation_matrix_diabetes[correlation_matrix_diabetes.abs() > 0.8].stack().index.tolist()
to_drop_diabetes = set()

for i, j in high_corr_pairs_diabetes:
    if i != j:
        to_drop_diabetes.add(j)
diabetes_cleaned_corr = diabetes_cleaned.drop(columns=to_drop_diabetes)
X_diabetes_corr = diabetes_cleaned_corr.drop(columns=['Outcome'])
y_diabetes_corr = diabetes_cleaned_corr['Outcome']
X_train_diabetes_corr, X_test_diabetes_corr, y_train_diabetes_corr, y_test_diabetes_corr = train_test_split(X_diabetes_corr, y_diabetes_corr, test_size=0.3, random_state=42)

model_diabetes_corr = LogisticRegression(max_iter=200)
model_diabetes_corr.fit(X_train_diabetes_corr, y_train_diabetes_corr)
y_pred_diabetes_corr = model_diabetes_corr.predict(X_test_diabetes_corr)
accuracy_diabetes_corr = accuracy_score(y_test_diabetes_corr, y_pred_diabetes_corr)
print(f'Accuracy after High Correlation Filter (Diabetes): {accuracy_diabetes_corr:.4f}')


X_diabetes = diabetes_cleaned.drop(columns=['Outcome'])
selector_diabetes = VarianceThreshold(threshold=0.01)
X_diabetes_low_variance = selector_diabetes.fit_transform(X_diabetes)


mask_diabetes = selector_diabetes.get_support()

low_variance_features = X_diabetes.loc[:, mask_diabetes]
diabetes_low_variance = pd.concat([low_variance_features, y_diabetes.reset_index(drop=True)], axis=1)




X_low_variance = diabetes_low_variance.drop(columns=['Outcome'])
y_low_variance = diabetes_low_variance['Outcome']
# Split the data into training and testing sets
X_low_variance_train, X_low_variance_test, y_low_variance_train, y_low_variance_test = train_test_split(X_low_variance, y_low_variance, test_size=0.3, random_state=42)

model_diabetes_low_variance = LogisticRegression(max_iter=200)
model_diabetes_low_variance.fit(X_low_variance_train, y_low_variance_train)
y_pred_diabetes_low_variance = model_diabetes_low_variance.predict(X_low_variance_test)

accuracy_diabetes_low_variance = accuracy_score(y_low_variance_test, y_pred_diabetes_low_variance)
print(f'Accuracy after Low Variance Filter (Diabetes): {accuracy_diabetes_low_variance:.4f}')


selector_rfecv_diabetes = RFECV(estimator=LogisticRegression(max_iter=200), step=1, cv=5)
selector_rfecv_diabetes.fit(X_diabetes, y_diabetes)
X_selected_diabetes = selector_rfecv_diabetes.transform(X_diabetes)
X_train_selected_diabetes, X_test_selected_diabetes, y_train_selected_diabetes, y_test_selected_diabetes = train_test_split(X_selected_diabetes, y_diabetes, test_size=0.3, random_state=42)


model_forward_diabetes = LogisticRegression(max_iter=200)
model_forward_diabetes.fit(X_train_selected_diabetes, y_train_selected_diabetes)
y_pred_forward_diabetes = model_forward_diabetes.predict(X_test_selected_diabetes)
accuracy_forward_diabetes = accuracy_score(y_test_selected_diabetes, y_pred_forward_diabetes)
print(f'Accuracy after Forward Feature Selection (Diabetes): {accuracy_forward_diabetes:.4f}')


remaining_features = X_diabetes.columns.tolist()


from sklearn.tree import DecisionTreeClassifier
# Use a Decision Tree Classifier for backward elimination
model_backward_diabetes = DecisionTreeClassifier()
# Train on the initial dataset
model_backward_diabetes.fit(X_train_diabetes, y_train_diabetes)
# Get initial feature importances and indices
importances_diabetes = model_backward_diabetes.feature_importances_
indices_diabetes = np.argsort(importances_diabetes)
# Convert features to list to keep track during elimination
remaining_features = list(X_diabetes.columns)
print(f"Initial Number of Features: {len(remaining_features)}")
while len(remaining_features) > 5:  # Stop when there are at least 5 features left
    # Drop the least important feature
    least_important_feature_index = indices_diabetes[0]
    feature_to_drop = remaining_features[least_important_feature_index]
    # Remove the feature from dataset and from remaining features list
    X_diabetes = X_diabetes.drop(columns=[feature_to_drop])
    remaining_features.remove(feature_to_drop)
    
    # Update the model
    X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(
        X_diabetes, y_diabetes, test_size=0.3, random_state=42)
    
    model_backward_diabetes.fit(X_train_diabetes, y_train_diabetes)
    importances_diabetes = model_backward_diabetes.feature_importances_
    indices_diabetes = np.argsort(importances_diabetes)

# Print final number of features
print(f"Final Number of Features After Elimination: {len(remaining_features)}")


importances_diabetes_sorted_indices = np.argsort(importances_diabetes)[::-1]  # Sort importances

# Get the number of features to print (up to 5)
num_features_to_print = min(5, len(remaining_features))

print("Top Important Features (Diabetes):")
for i in range(num_features_to_print):
    if i < len(importances_diabetes):
        feature_name = remaining_features[importances_diabetes_sorted_indices[i]]
        importance_value = importances_diabetes[importances_diabetes_sorted_indices[i]]
        print(f"{i + 1}. Feature '{feature_name}': {importance_value:.4f}")



Melbourne Housing Dataset:

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import VarianceThreshold, RFE, SelectFromModel
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd


melbourne_data = pd.read_csv('melbourne_housing_raw.csv')


missing_percentage = melbourne_data.isnull().mean() * 100
columns_to_remove = [col for col in missing_percentage.index if missing_percentage[col] > 20 and col != 'Price']
reduced_data = melbourne_data.drop(columns=columns_to_remove)




reduced_data = reduced_data.dropna(subset=['Price'])
X = reduced_data.drop(columns=['Price', 'Date', 'Suburb', 'Type', 'Method', 'SellerG', 'CouncilArea', 'Regionname'])
y = reduced_data['Price']


X.fillna(X.mean(), inplace=True)
# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


corr_matrix = X_train.corr().abs()
# Identify pairs of highly correlated features
high_corr_pairs = np.where(corr_matrix > 0.85)
high_corr_features = set([X_train.columns[i] for i in high_corr_pairs[0] if i != high_corr_pairs[1][i]])



X_train_corr_filtered = X_train.drop(columns=high_corr_features)
X_test_corr_filtered = X_test.drop(columns=high_corr_features)



variance_filter = VarianceThreshold(threshold=0.01)  # Example threshold for low variance
X_train_low_var = variance_filter.fit_transform(X_train_corr_filtered)
X_test_low_var = variance_filter.transform(X_test_corr_filtered)



linear_model = LinearRegression()
rfe_selector = RFE(estimator=linear_model, n_features_to_select=5, step=1)
rfe_selector.fit(X_train_low_var, y_train)
X_train_forward = rfe_selector.transform(X_train_low_var)
X_test_forward = rfe_selector.transform(X_test_low_var)


random_forest_model = RandomForestRegressor(random_state=42)
rfe_backward = RFE(estimator=random_forest_model, n_features_to_select=5, step=1)
rfe_backward.fit(X_train_low_var, y_train)
X_train_backward = rfe_backward.transform(X_train_low_var)
X_test_backward = rfe_backward.transform(X_test_low_var)


rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
important_features = SelectFromModel(rf_model, threshold="mean", prefit=True)
X_train_rf_selected = important_features.transform(X_train)
X_test_rf_selected = important_features.transform(X_test)



def evaluate_model(X_train, X_test, y_train, y_test):
    model = RandomForestRegressor(random_state=42)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    return mse

results = {
    "Baseline (No Feature Selection)": evaluate_model(X_train, X_test, y_train, y_test),
    "High Correlation Filter": evaluate_model(X_train_corr_filtered, X_test_corr_filtered, y_train, y_test),
    "Low Variance Filter": evaluate_model(X_train_low_var, X_test_low_var, y_train, y_test),
    "Forward Selection": evaluate_model(X_train_forward, X_test_forward, y_train, y_test),
    "Backward Elimination": evaluate_model(X_train_backward, X_test_backward, y_train, y_test),
    "Random Forest Selection": evaluate_model(X_train_rf_selected, X_test_rf_selected, y_train, y_test)
}
